{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip as clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import translators as ts\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load CLIP Model for Tagging\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Function to Generate Tags for Images\n",
    "def generate_tags(image_folder):\n",
    "    tags_dict = {}\n",
    "    for img_file in os.listdir(image_folder):\n",
    "        image_path = os.path.join(image_folder, img_file)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image_tensor = preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "        # Detect objects using CLIP (or substitute with YOLO/Detectron if needed)\n",
    "        with torch.no_grad():\n",
    "            text_tags = [\n",
    "                \"woman\",\n",
    "                \"cat\",\n",
    "                \"sofa\",\n",
    "                \"shoe\",\n",
    "            ]  # Add more tags based on detected objects\n",
    "            text_inputs = clip.tokenize(text_tags).to(device)\n",
    "            logits_per_image, logits_per_text = model(image_tensor, text_inputs)\n",
    "            probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "        # Filter tags based on threshold (e.g., > 0.5 probability)\n",
    "        selected_tags = [tag for i, tag in enumerate(text_tags) if probs[0][i] > 0.5]\n",
    "        tags_dict[img_file] = selected_tags\n",
    "\n",
    "    return tags_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Translate Hinglish Query to English\n",
    "def translate_query(hindi_text):\n",
    "    return ts.translate_text(\n",
    "        hindi_text, translator=\"google\", from_language=\"hi\", to_language=\"en\"\n",
    "    )\n",
    "    # return ts.google(hindi_text, from_language=\"auto\", to_language=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Embedding Model for Multilingual Similarity (LaBSE or Similar)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/LaBSE\")\n",
    "embedding_model = AutoModel.from_pretrained(\"sentence-transformers/LaBSE\").to(device)\n",
    "\n",
    "def get_embedding(text):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        embedding = embedding_model(**tokens).pooler_output.cpu().numpy()\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Query Processing and Retrieval\n",
    "def retrieve_images(query, tags_dict):\n",
    "    # Translate and embed query\n",
    "    translated_query = translate_query(query)\n",
    "    query_embedding = get_embedding(translated_query)\n",
    "\n",
    "    # Calculate cosine similarity between query and tags\n",
    "    results = {}\n",
    "    for img_file, tags in tags_dict.items():\n",
    "        tag_embeddings = np.vstack([get_embedding(tag) for tag in tags])\n",
    "        similarity_scores = cosine_similarity(query_embedding, tag_embeddings).flatten()\n",
    "        results[img_file] = max(similarity_scores)\n",
    "\n",
    "    # Sort results by highest similarity\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [img for img, score in sorted_results[:1]]  # Retrieve top 5 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top retrieved images: ['2.webp']\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "image_folder = \"D:\\SNU\\Semester VII\\CSD358 Information Retrieval\\Project\\images\"\n",
    "tags_dict = generate_tags(image_folder)\n",
    "query = \"ladki ka photo\"\n",
    "top_images = retrieve_images(query, tags_dict)\n",
    "print(\"Top retrieved images:\", top_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top retrieved images: ['2.webp']\n"
     ]
    }
   ],
   "source": [
    "query = \"shoes\"\n",
    "top_images = retrieve_images(query, tags_dict)\n",
    "print(\"Top retrieved images:\", top_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
