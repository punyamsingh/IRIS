{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\punya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\punya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\punya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\punya\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption for 1.webp: two women in red and yellow dresses posing for the camera\n",
      "Tags for 1.webp: ['crop', 'plume', 'adult female', 'photographic camera', 'women', 'coiffure', 'television camera', 'coiffe', 'trim', 'curry', 'set', 'tv camera', 'dress', 'cut back', 'garb', 'habilitate', 'preen', 'woman', 'get dressed', 'attire', 'clothe', 'arrange', 'wearing apparel', 'tog', 'dress up', 'dresses', 'cleaning lady', 'cleaning woman', 'apparel', 'camera', 'decorate', 'primp', 'frock', 'garnish', 'garment', 'snip', 'womanhood', 'enclothe', 'coif', 'fair sex', 'clip', 'do', 'lop', 'prune', 'groom', 'fit out', 'line up', 'charwoman', 'raiment', 'clothes', 'char', 'dress out']\n",
      "Caption for 2.webp: a pair of white shoes on a blue and pink background\n",
      "Tags for 2.webp: ['wild blue yonder', 'juicy', 'bluing', 'pinko', 'dispirited', 'shoe', 'blueing', 'twain', 'twosome', 'brace', 'background knowledge', 'mate', 'couplet', 'geminate', 'backcloth', 'blue air', 'duo', 'ping', 'screen background', 'risque', 'ground', 'down', 'blueness', 'dark', 'pinkish', 'setting', 'scope', 'place', 'couple', 'tap', 'profane', 'drab', 'blue-blooded', 'dreary', 'drear', 'rap', 'yoke', 'racy', 'backdrop', 'background', 'sorry', 'low', 'gloomy', 'match', 'pair', 'grim', 'amobarbital sodium', 'dyad', 'down in the mouth', 'downcast', 'blasphemous', 'patrician', 'shoes', 'horseshoe', 'aristocratical', 'low-spirited', 'distich', 'blue devil', 'pink', 'downplay', 'knock', 'blue', 'depressed', 'spicy', 'duad', 'brake shoe', 'blue angel', 'skid', 'play down', 'disconsolate', 'blueish', 'gentle', 'naughty', 'twin', 'partner off', 'blue sky', 'copulate', 'span', 'desktop', 'duet', 'Amytal', 'downhearted', 'garden pink', 'pair off', 'puritanic', 'gamy', 'gamey', 'background signal', 'bluish', 'puritanical', 'dingy', 'dismal', 'aristocratic']\n",
      "Updated data saved to image_tags.json\n",
      "Top retrieved images: ['1.webp']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from nltk.corpus import wordnet\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Download NLTK data if not already downloaded\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Load CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Load BLIP model for captioning\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "caption_model = BlipForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip-image-captioning-base\"\n",
    ").to(device)\n",
    "\n",
    "# File to store image captions and tags\n",
    "DATA_FILE = \"image_tags.json\"\n",
    "\n",
    "# Load or initialize the JSON file for storing processed images\n",
    "if os.path.exists(DATA_FILE):\n",
    "    with open(DATA_FILE, \"r\") as file:\n",
    "        image_data = json.load(file)\n",
    "else:\n",
    "    image_data = {}\n",
    "\n",
    "\n",
    "# Function to generate caption for an image\n",
    "def generate_caption(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "    out = caption_model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "\n",
    "# Function to extract nouns from text\n",
    "def extract_nouns(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    nouns = [word for word, pos in pos_tags if pos.startswith(\"NN\")]\n",
    "    return nouns\n",
    "\n",
    "\n",
    "# Function to find synonyms of a word using WordNet\n",
    "def find_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(\n",
    "                lemma.name().replace(\"_\", \" \")\n",
    "            )  # Replace underscores with spaces for readability\n",
    "    return list(synonyms)\n",
    "\n",
    "\n",
    "# Function to generate tags with synonyms for new images\n",
    "def generate_tags_with_synonyms(image_folder):\n",
    "    for img_file in os.listdir(image_folder):\n",
    "        if img_file.lower().endswith((\"jpg\", \"jpeg\", \"png\", \"webp\")):\n",
    "            image_path = os.path.join(image_folder, img_file)\n",
    "\n",
    "            # Skip images that are already processed and stored in the JSON file\n",
    "            if img_file in image_data:\n",
    "                print(f\"Skipping already processed image: {img_file}\")\n",
    "                continue\n",
    "\n",
    "            # Step 1: Generate caption for the image\n",
    "            caption = generate_caption(image_path)\n",
    "            print(f\"Caption for {img_file}: {caption}\")\n",
    "\n",
    "            # Step 2: Extract nouns from the caption\n",
    "            initial_tags = extract_nouns(caption)\n",
    "            expanded_tags = set()\n",
    "\n",
    "            # Step 3: Expand each noun with synonyms and add to tag set\n",
    "            for tag in initial_tags:\n",
    "                expanded_tags.add(tag)  # Include the original noun\n",
    "                synonyms = find_synonyms(tag)\n",
    "                expanded_tags.update(synonyms)  # Add synonyms to the tag set\n",
    "\n",
    "            # Store the caption and tags in the image data dictionary\n",
    "            image_data[img_file] = {\"caption\": caption, \"tags\": list(expanded_tags)}\n",
    "            print(f\"Tags for {img_file}: {image_data[img_file]['tags']}\")\n",
    "\n",
    "    # Save updated image data to the JSON file\n",
    "    with open(DATA_FILE, \"w\") as file:\n",
    "        json.dump(image_data, file, indent=4)\n",
    "    print(f\"Updated data saved to {DATA_FILE}\")\n",
    "\n",
    "\n",
    "# Function to retrieve images based on free-text query\n",
    "def retrieve_images(query):\n",
    "    # Tokenize and normalize the query (convert to lowercase)\n",
    "    query_tokens = set(nltk.word_tokenize(query.lower()))\n",
    "    results = {}\n",
    "\n",
    "    # Check each image's tags to see if they match the query tokens\n",
    "    for img_file, data in image_data.items():\n",
    "        tags = set(tag.lower() for tag in data[\"tags\"])\n",
    "        common_tags = query_tokens.intersection(tags)\n",
    "        if common_tags:\n",
    "            # Count of matching tags can be used as a score\n",
    "            results[img_file] = len(common_tags)\n",
    "\n",
    "    # Sort images by the number of matching tags (descending order)\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_images = [img for img, score in sorted_results]\n",
    "\n",
    "    return top_images\n",
    "\n",
    "\n",
    "# Usage Example\n",
    "image_folder = \"images\"  # Replace with your actual image folder path\n",
    "generate_tags_with_synonyms(image_folder)\n",
    "\n",
    "# Free-text query example\n",
    "query = \"woman in red dress\"\n",
    "top_images = retrieve_images(query)\n",
    "print(\"Top retrieved images:\", top_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"female\"\n",
    "top_images = retrieve_images(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
