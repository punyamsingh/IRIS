{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\punya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\punya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\punya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Download NLTK data if not already downloaded\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Load the BLIP model for captioning\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "caption_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "\n",
    "# File to store image captions and tags\n",
    "DATA_FILE = \"image_tags.json\"\n",
    "\n",
    "# Load or initialize the JSON file for storing processed images\n",
    "if os.path.exists(DATA_FILE):\n",
    "    with open(DATA_FILE, \"r\") as file:\n",
    "        image_data = json.load(file)\n",
    "else:\n",
    "    image_data = {}\n",
    "\n",
    "# Function to generate caption for an image\n",
    "def generate_caption(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "    out = caption_model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "# Function to extract nouns from text\n",
    "def extract_nouns(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    nouns = [word for word, pos in pos_tags if pos.startswith(\"NN\")]\n",
    "    return nouns\n",
    "\n",
    "# Function to find a large set of related words for a tag using WordNet\n",
    "def expand_with_synonyms(tag, min_tags=200):\n",
    "    related_words = set()\n",
    "    \n",
    "    # Get synonyms, hypernyms, hyponyms, and related words\n",
    "    for syn in wordnet.synsets(tag):\n",
    "        for lemma in syn.lemmas():\n",
    "            related_words.add(lemma.name().replace('_', ' '))  # Include synonyms\n",
    "        for hypernym in syn.hypernyms():\n",
    "            related_words.update(lemma.name().replace('_', ' ') for lemma in hypernym.lemmas())\n",
    "        for hyponym in syn.hyponyms():\n",
    "            related_words.update(lemma.name().replace('_', ' ') for lemma in hyponym.lemmas())\n",
    "    \n",
    "    # Ensure we have a minimum of `min_tags` related words\n",
    "    if len(related_words) < min_tags:\n",
    "        related_words.update(list(related_words)[:min_tags - len(related_words)])\n",
    "    \n",
    "    return list(related_words)[:min_tags]  # Return up to `min_tags`\n",
    "\n",
    "# Function to generate a large tag set for new images\n",
    "def generate_tags_with_expansion(image_folder, min_tags=200):\n",
    "    for img_file in os.listdir(image_folder):\n",
    "        if img_file.lower().endswith(('jpg', 'jpeg', 'png', 'webp')):\n",
    "            image_path = os.path.join(image_folder, img_file)\n",
    "            \n",
    "            # Skip images that are already processed and stored in the JSON file\n",
    "            if img_file in image_data:\n",
    "                print(f\"Skipping already processed image: {img_file}\")\n",
    "                continue\n",
    "\n",
    "            # Step 1: Generate caption for the image\n",
    "            caption = generate_caption(image_path)\n",
    "            print(f\"Caption for {img_file}: {caption}\")\n",
    "\n",
    "            # Step 2: Extract nouns from the caption\n",
    "            initial_tags = extract_nouns(caption)\n",
    "            expanded_tags = set()\n",
    "\n",
    "            # Step 3: Expand each noun with a large set of related words\n",
    "            for tag in initial_tags:\n",
    "                related_words = expand_with_synonyms(tag, min_tags=min_tags // len(initial_tags))\n",
    "                expanded_tags.update(related_words)\n",
    "\n",
    "            # Store the caption and tags in the image data dictionary\n",
    "            image_data[img_file] = {\n",
    "                \"caption\": caption,\n",
    "                \"tags\": list(expanded_tags)\n",
    "            }\n",
    "            print(f\"Tags for {img_file}: {len(image_data[img_file]['tags'])} tags generated.\")\n",
    "\n",
    "    # Save updated image data to the JSON file\n",
    "    with open(DATA_FILE, \"w\") as file:\n",
    "        json.dump(image_data, file, indent=4)\n",
    "    print(f\"Updated data saved to {DATA_FILE}\")\n",
    "\n",
    "# Function to expand the search query with related words\n",
    "def expand_query(query):\n",
    "    query_tokens = nltk.word_tokenize(query.lower())\n",
    "    expanded_query = set()\n",
    "\n",
    "    # Add synonyms and related words for each query word\n",
    "    for word in query_tokens:\n",
    "        expanded_query.add(word)\n",
    "        synonyms = expand_with_synonyms(word, min_tags=20)  # Limit query expansion to 20 words per term\n",
    "        expanded_query.update(synonyms)\n",
    "    \n",
    "    return expanded_query\n",
    "\n",
    "# Function to retrieve images based on expanded query\n",
    "def retrieve_images(query):\n",
    "    expanded_query = expand_query(query)\n",
    "    results = {}\n",
    "\n",
    "    # Check each image's tags to see if they match the expanded query terms\n",
    "    for img_file, data in image_data.items():\n",
    "        tags = set(tag.lower() for tag in data[\"tags\"])\n",
    "        common_tags = expanded_query.intersection(tags)\n",
    "        if common_tags:\n",
    "            # Count of matching tags can be used as a score\n",
    "            results[img_file] = len(common_tags)\n",
    "\n",
    "    # Sort images by the number of matching tags (descending order)\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_images = [img for img, score in sorted_results]\n",
    "    \n",
    "    return top_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping already processed image: 1.webp\n",
      "Skipping already processed image: 2.webp\n",
      "Skipping already processed image: 3.jpg\n",
      "Skipping already processed image: 360_F_555879079_fg6dTHFT29m5B7qgWei45WbYYle3pCFA.jpg\n",
      "Skipping already processed image: Designer2.png\n",
      "Skipping already processed image: IMG-20230113-WA0352.jpg\n",
      "Skipping already processed image: redfront.jpg\n",
      "Skipping already processed image: WhatsApp Image 2022-11-03 at 17.24.45.jpeg\n",
      "Updated data saved to image_tags.json\n",
      "Top retrieved images: ['3.jpg', 'IMG-20230113-WA0352.jpg', '2.webp']\n"
     ]
    }
   ],
   "source": [
    "# Usage Example\n",
    "image_folder = \"images\"  # Replace with your actual image folder path\n",
    "generate_tags_with_expansion(image_folder, min_tags=200)\n",
    "\n",
    "# Free-text query example\n",
    "query = \"two men\"\n",
    "top_images = retrieve_images(query)\n",
    "print(\"Top retrieved images:\", top_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
